<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Document</title>
</head>
<body>
    <h1>Artificial intelligence</h1>
    <hr/>
    <p><i>From Wikipedia, the free encyclopedia</i></p>
    <p><i>AI" redirects here. For other uses, see AI 
    (disambiguation), Artificial intelligence (disambiguation), and Intelligent agent.</i></p>
    <img style="float: right;" height="300" src="https://upload.wikimedia.org/wikipedia/commons/thumb/1/1c/Artificial_intelligence_prompt_completion_by_dalle_mini.jpg/150px-Artificial_intelligence_prompt_completion_by_dalle_mini.jpg" alt="AI image">
    <p><b>Artificial intelligence</b> (AI) is intelligence—perceiving, synthesizing, 
        and inferring<br/> information—demonstrated by machines, as opposed to intelligence 
        displayed by<br/> humans or by other animals. "Intelligence" 
        encompasses the ability to learn and to <br/> reason, to generalize, and to infer 
        meaning.<sup>[1]</sup> Example tasks in which this is done include <br/>speech recognition, 
        computer vision, translation between (natural) languages, as well as<br/> other
        mappings of inputs.<sup>[2]</sup>
    </p>
    <p>AI applications include advanced web search engines (e.g., Google Search),<br>/
        recommendation systems (used by YouTube, Amazon, and Netflix), understanding<br/>
        human speech (such as Siri and Alexa), self-driving cars (e.g., Waymo), generative or<br/> 
        creative tools (ChatGPT and AI art), automated decision-making, and competing at the<br/>
        highest level in strategic game systems (such as chess and Go).<sup>[3]</sup><br/>
        <br/>
        As machines become increasingly capable, tasks considered to require "intelligence" are <br/>
        often removed fromthe definition of AI, a phenomenon known as the AI effect.<sup>[4]</sup> For <br/>
        instance, optical character recognition is frequently excluded from things considered to be <br/>
        AI, having become a routine technology.<sup>[5]</sup>
    </p>
    <p>
        Artificial intelligence was founded as an academic discipline in 1956, and in the years<br/>
        since it has experienced several waves of optimism,<sup>[6][7]</sup> followed by disappointment and <br/>
        the loss of funding (known as an "AI winter"),<sup>[8][9]</sup> followed by new approaches, success, and renewed funding <sup>[7][10]</sup> AI research<br/>
        has tried and discarded many different approaches, including simulating the brain, modeling human problem solving, formal logic, large <br/>
        databases of knowledge, and imitating animal behavior. In the first decades of the 21st century, highly mathematical and statistical <br/>
        machine learning has dominated the field, and this technique has proved highly successful, helping to solve many challenging <br/>
        problems throughout industry and academia.<sup>[10][11]</sup>
    </p>
    <p>
        The various sub-fields of AI research are centered around particular goals and the use of particular tools. The traditional goals of AI<br/>
        research include reasoning, knowledge representation, planning, learning, natural language processing, perception, and the ability to<br/>
        move and manipulate objects.<sup>[a]</sup> General intelligence (the ability to solve an arbitrary problem) is among the field's long-term goals.<sup>[12]</sup><br/>
        To solve these problems, AI researchers have adapted and integrated a wide range of problem-solving techniques, including search <br/>
        and mathematical optimization, formal logic, artificial neural networks, and methods based on statistics, probability, and economics. AI<br/>
         also draws upon computer science, psychology, linguistics, philosophy, and many other fields.
    </p>
    <p>
        The field was founded on the assumption that human intelligence "can be so precisely described that a machine can be made to<br/>
        simulate it".<sup>[b]</sup> This raised philosophical arguments about the mind and the ethical consequences of creating artificial beings endowed<br/>
        with human-like intelligence; these issues have previously been explored by myth, fiction (science fiction), and philosophy since<br/>
        antiquity.<sup>[14]</sup> Computer scientists and philosophers have since suggested that AI may become an existential risk to humanity if its<br/>
        rational capacities are not steered towards goals beneficial to humankind.<sup>[c]</sup> Economists have frequently highlighted the risks of<br/>
        redundancies from AI, and speculated about unemployment if there is no adequate social policy for full employment.<sup>[15]</sup> The term<br/>
        artificial intelligence has also been criticized for overhyping AI's true technological capabilities.<sup>[16][17][18]</sup>
    </p>
    <h2>History</h2>
    <hr/>
    <p><i>Main articles: History of artificial intelligence and Timeline of artificial intelligence</i></p>
    <img style="float: right;" src="images1/ai.jpg" alt="Silver didrachma">
    <p>
        Artificial beings with intelligence appeared as storytelling devices in antiquity,<sup>[19]</sup> and have<br/>
        been common in fiction, as in Mary Shelley's Frankenstein or Karel Čapek's R.U.R.<sup>[20]</sup>These<br/> 
        characters and their fates raised many of the same issues now discussed in the ethics of artificial intelligence.<sup>[21]</sup> 
    </p>
    <p>
        The study of mechanical or "formal" reasoning began with philosophers and mathematicians in <br/>
        antiquity. The study of mathematical logic led directly to Alan Turing's theory of computation,<br/>
        which suggested that a machine, by shuffling symbols as simple as "0" and "1", could simulate<br/>
        any conceivable act of mathematical deduction. This insight that digital computers can simulate<br/> 
        any process of formal reasoning is known as the Church–Turing thesis.<sup>[22]</sup> This, along with<br/> 
        concurrent discoveries in neurobiology, information theory and cybernetics, led researchers to <br/>
        consider the possibility of building an electronic brain.<sup>[23]</sup> The first work that is now generally<br/>
         recognized as AI was McCullouch and Pitts' 1943 formal design for Turing-complete "artificial neurons".<sup>[24]</sup>
    </p>
    <p>
        By the 1950s, two visions for how to achieve machine intelligence emerged. One vision,<br/> 
        known as Symbolic AI or GOFAI, was to use computers to create a symbolic representation of the world and systems that could reason about<br/> 
        the world. Proponents included Allen Newell, Herbert A. Simon, and Marvin Minsky. Closely associated with this approach was the<br/>
        "heuristic search" approach, which likened intelligence to a problem of exploring a space of possibilities for answers.
    </p>
    <p>
        The second vision, known as the connectionist approach, sought to achieve intelligence through learning. Proponents of this approach,<br/> 
        most prominently Frank Rosenblatt, sought to connect Perceptron in ways inspired by connections of neurons.<sup>[25]</sup> James Manyika and<br/> 
        others have compared the two approaches to the mind (Symbolic AI) and the brain (connectionist). Manyika argues that symbolic<br/>
        approaches dominated the push for artificial intelligence in this period, due in part to its connection to intellectual traditions of<br/>
        Descartes, Boole, Gottlob Frege, Bertrand Russell, and others. Connectionist approaches based on cybernetics or artificial neural<br/>
        networks were pushed to the background but have gained new prominence in recent decades.<sup>[26]</sup>
    </p>
    <p>
        The field of AI research was born at a workshop at Dartmouth College in 1956.<sup>[d][29]</sup> The attendees became the founders and leaders of<br/>
        AI research.[e] They and their students produced programs that the press described as "astonishing":[f] computers were learning <br/>
        checkers strategies, solving word problems in algebra, proving logical theorems and speaking English.<sup>[g][31]</sup>
    </p>
    <P>
        By the middle of the 1960s, research in the U.S. was heavily funded by the Department of Defense[32] and laboratories had been <br/>
        established around the world.<sup>[33]</sup>
    </P>
    <p>
        Researchers in the 1960s and the 1970s were convinced that symbolic approaches would eventually succeed in creating a machine <br/>
        with artificial general intelligence and considered this the goal of their field.[34] Herbert Simon predicted, "machines will be capable,<br/> 
        within twenty years, of doing any work a man can do".<sup>[35]</sup> Marvin Minsky agreed, writing, "within a generation ... the problem of creating <br/>
        'artificial intelligence' will substantially be solved".<sup>[36]</sup>
    </p>
    <p>
        They had failed to recognize the difficulty of some of the remaining tasks. Progress slowed and in 1974, in response to the criticism of<br/> 
        Sir James Lighthill<sup>[37]</sup> and ongoing pressure from the US Congress to fund more productive projects, both the U.S. and British<br/> 
        governments cut off exploratory research in AI. The next few years would later be called an "AI winter", a period when obtaining funding<br/> 
        for AI projects was difficult.<sup>[8]</sup>
    </p>
    <p>
        In the early 1980s, AI research was revived by the commercial success of expert systems,<sup>[38]</sup> a form of AI program that simulated the <br/>
        knowledge and analytical skills of human experts. By 1985, the market for AI had reached over a billion dollars. At the same time, <br/>
        Japan's fifth generation computer project inspired the U.S. and British governments to restore funding for academic research.<sup>[7]</sup> <br/>
        However, beginning with the collapse of the Lisp Machine market in 1987, AI once again fell into disrepute, and a second, longer-lasting<br/>
        winter began.<sup>[9]</sup>
    </p>
    <p>
        Many researchers began to doubt that the symbolic approach would be able to imitate all the processes of human cognition, especially<br/> 
        perception, robotics, learning and pattern recognition. A number of researchers began to look into "sub-symbolic" approaches to <br/>
        specific AI problems.<sup>[39]</sup> Robotics researchers, such as Rodney Brooks, rejected symbolic AI and focused on the basic engineering <br/>
        problems that would allow robots to move, survive, and learn their environment.<sup>[h]</sup>
    </p>
    <p>
        Interest in neural networks and "connectionism" was revived by Geoffrey Hinton, David Rumelhart and others in the middle of the <br/>
        1980s.<sup>[44]</sup> Soft computing tools were developed in the 1980s, such as neural networks, fuzzy systems, Grey system theory, <br/>
        evolutionary computation and many tools drawn from statistics or mathematical optimization.
    </p>
    <p>
        AI gradually restored its reputation in the late 1990s and early 21st century by finding specific solutions to specific problems. The <br/>
        narrow focus allowed researchers to produce verifiable results, exploit more mathematical methods, and collaborate with other fields<br/> 
        (such as statistics, economics and mathematics).<sup>[45]</sup> By 2000, solutions developed by AI researchers were being widely used, although<br/> 
        in the 1990s they were rarely described as "artificial intelligence".<sup>[11]</sup>
    </p>
    <p>
        Faster computers, algorithmic improvements and access to large amounts of data enabled advances in machine learning and <br/>
        perception; data-hungry deep learning methods started to dominate accuracy benchmarks around 2012.<sup>[46]</sup> According to Bloomberg's <br/>
        Jack Clark, 2015 was a landmark year for artificial intelligence, with the number of software projects that use AI within Google <br/>
        increased from a "sporadic usage" in 2012 to more than 2,700 projects.<sup>[i]</sup> He attributed this to an increase in affordable neural<br/> 
        networks, due to a rise in cloud computing infrastructure and to an increase in research tools and datasets.<sup>[10]</sup>
    </p>
    <p>
        In a 2017 survey, one in five companies reported they had "incorporated AI in some offerings or processes".<sup>[47]</sup> The amount of research<br/> 
        into AI (measured by total publications) increased by 50% in the years 2015–2019.<sup>[48]</sup> According to AI Impacts at Stanford, around 2022<br/>
        about $50 billion annually is invested in artificial intelligence in the US, and about 20% of new US Computer Science PhD graduates<br/> 
        have specialized in artificial intelligence;[49] about 800,000 AI-related US job openings existed in 2022.<sup>[50]</sup>
    </p>
    <p>
        Numerous academic researchers became concerned that AI was no longer pursuing the original goal of creating versatile, fully <br/>
        intelligent machines. Much of current research involves statistical AI, which is overwhelmingly used to solve specific problems, even <br/>
        highly successful techniques such as deep learning. This concern has led to the subfield of artificial general intelligence (or "AGI"), <br/>
        which had several well-funded institutions by the 2010s.<sup>[12]</sup>
    </p>
    <h2>Goals</h2>
    <hr/>
    <p>
        The general problem of simulating (or creating) intelligence has been broken down into sub-problems. These consist of particular traits <br/>
        or capabilities that researchers expect an intelligent system to display. The traits described below have received the most attention.<sup>[a]</sup>
    </p>
    <h3><b>Reasoning, problem-solving</b></h3>
    <p>
        Early researchers developed algorithms that imitated step-by-step reasoning that humans use when they solve puzzles or make logical <br/>
        deductions.[51] By the late 1980s and 1990s, AI research had developed methods for dealing with uncertain or incomplete information, <br/>
        employing concepts from probability and economics.<sup>[52]</sup>
    </p>
    <p>
        Many of these algorithms proved to be insufficient for solving large reasoning problems because they experienced a "combinatorial <br/>
        explosion": they became exponentially slower as the problems grew larger.<sup>[53]</sup> Even humans rarely use the step-by-step deduction that <br/>
        early AI research could model. They solve most of their problems using fast, intuitive judgments.<sup>[54]</sup>
    </p>
    <h3><b>Knowledge representation</b></h3>
    <p><i>Main articles: Knowledge representation, Commonsense knowledge, Description logic, and Ontology</i></p>
    <p>
        Knowledge representation and knowledge engineering<sup>[55]</sup> allow AI programs to answer <br/>
        questions intelligently and make deductions about real-world facts.
    </p>
    <p>
        A representation of "what exists" is an ontology: the set of objects, relations, concepts, and <br/>
        properties formally described so that software agents can interpret them.<sup>[56]</sup> The most general <br/>
        ontologies are called upper ontologies, which attempt to provide a foundation for all other <br/>
        knowledge and act as mediators between domain ontologies that cover specific knowledge <br/>
        about a particular knowledge domain (field of interest or area of concern). A truly intelligent <br/>
        program would also need access to commonsense knowledge, the set of facts that an average <br/>
        person knows. The semantics of an ontology is typically represented in description logic, such <br/>
        as the Web Ontology Language.<sup>[57]</sup>
    </p>
    <p>
        AI research has developed tools to represent specific domains, such as objects, properties, <br/>
        categories and relations between objects;[57] situations, events, states and time;<sup>[58]</sup> causes <br/>
        and effects;<sup>[59]</sup> knowledge about knowledge (what we know about what other people know); <br/>
        and <sup>[60]</sup> default reasoning (things that humans assume are true until they are told differently <br/>
        and will remain true even when other facts are changing);<sup>[61]</sup>. Among the most difficult problems in AI are: the breadth of commonsense <br/>
        knowledge (the number of atomic facts that the average person knows is enormous);<sup>[62]</sup> and the sub-symbolic form of most <br/>
        commonsense knowledge (much of what people know is not represented as "facts" or "statements" that they could express <br/>
        verbally).<sup>[54]</sup>
    </p>
    <p>
        Formal knowledge representations are used in content-based indexing and retrieval,<sup>[63]</sup> scene interpretation,<sup>[64]</sup> clinical decision <br/>
        support,<sup>[65]</sup> knowledge discovery (mining "interesting" and actionable inferences from large databases),<sup>[66]</sup> and other areas.<sup>[67]</sup>
    </p>
    <h3><b>Learning</b></h3>
    <p><i>Main article: Machine learning</i></p>
    <p>
        Machine learning (ML), a fundamental concept of AI research since the field's inception,<sup>[j]</sup> is the study of computer algorithms that <br/>
        improve automatically through experience.<sup>[k]</sup>
    </p>
    <p>Unsupervised learning finds patterns in a stream of input.</p>
    <p>
        Supervised learning requires a human to label the input data first, and comes in two main varieties: classification and numerical <br/>
        regression. Classification is used to determine what category something belongs in – the program sees a number of examples of things <br/>
        from several categories and will learn to classify new inputs. Regression is the attempt to produce a function that describes the <br/>
        relationship between inputs and outputs and predicts how the outputs should change as the inputs change. Both classifiers and <br/>
        regression learners can be viewed as "function approximators" trying to learn an unknown (possibly implicit) function; for example, a <br/>
        spam classifier can be viewed as learning a function that maps from the text of an email to one of two categories, "spam" or "not <br/>
        spam".<sup>[71]</sup>
    </p>
    <p>
        In reinforcement learning the agent is rewarded for good responses and punished for bad ones. The agent classifies its responses to <br/>
        form a strategy for operating in its problem space.<sup>[72]</sup>
    </p>
    <p>Transfer learning is when the knowledge gained from one problem is applied to a new problem.<sup>[73]</sup></p>
    <p>
        Computational learning theory can assess learners by computational complexity, by sample complexity (how much data is required), or <br/>
        by other notions of optimization.<sup>[74]</sup>
    </p>
</body>
</html>